# GitHub Personal Access Token for collecting repositories
# Required for scripts/collect_github_repos.py
# Create at: https://github.com/settings/tokens (needs repo:read scope)
GITHUB_PAT=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Hugging Face Token for model downloads and pushing
# Required for downloading gated models and pushing to Hub
# Create at: https://huggingface.co/settings/tokens
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Directory paths
DATA_DIR=./data
OUTPUT_DIR=./outputs
MODEL_DIR=./models

# HuggingFace cache location (for vLLM/transformers model downloads)
# Uncomment to store models in project directory instead of ~/.cache/huggingface
# HF_HOME=./models/huggingface

# Model configuration
# GPT-OSS-20B fits comfortably in 16GB+ VRAM
MODEL_ID=unsloth/gpt-oss-20b

# vLLM server configuration
VLLM_PORT=8000
VLLM_HOST=0.0.0.0
VLLM_GPU_MEMORY_UTILIZATION=0.90
VLLM_MAX_MODEL_LEN=16384  # 16K context, ~24GB VRAM. Use 8192 for tighter memory
VLLM_SERVED_NAME=gpt-oss

# Tool calling configuration (for agent workflows)
VLLM_ENABLE_TOOL_CHOICE=true
VLLM_TOOL_CALL_PARSER=hermes
# VLLM_REASONING_PARSER=deepseek_r1  # Uncomment for reasoning/thinking models

# Node.js version for repo-based evaluation tasks
NODE_VERSION=24

# Evaluator configuration
EVAL_API_KEY=EMPTY

# Training configuration
TRAIN_BATCH_SIZE=1
TRAIN_GRADIENT_ACCUMULATION_STEPS=8
TRAIN_LEARNING_RATE=2e-5
TRAIN_NUM_EPOCHS=3

# QLoRA configuration
QLORA_R=64
QLORA_ALPHA=16
QLORA_DROPOUT=0.05

# Wandb (optional, for experiment tracking)
WANDB_PROJECT=gptoss-ts-posttrain
WANDB_ENTITY=
WANDB_API_KEY=
