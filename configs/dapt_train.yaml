# Domain-Adaptive Pre-Training Configuration
# Used by: scripts/run_dapt_train.py

# Base model (Unsloth's gpt-oss-20b for QLoRA)
# Unsloth handles quantization internally - use the base model
model_id: "unsloth/gpt-oss-20b"

# Training data
train_data: "./data/corpus/train_corpus.jsonl"
output_dir: "./outputs/checkpoints/dapt"

# LoRA Configuration (QLoRA via Unsloth)
lora:
  r: 64                    # LoRA rank
  alpha: 16                # LoRA alpha (scaling)
  dropout: 0.0             # Unsloth optimized (0 dropout)
  target_modules:          # Modules to apply LoRA to
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# Training hyperparameters (optimized for Unsloth)
training:
  batch_size: 2                      # Per-device batch size (can be larger with Unsloth)
  gradient_accumulation_steps: 4     # Effective batch = 8
  learning_rate: 2.0e-4              # Higher LR works with Unsloth
  num_epochs: 3                      # Training epochs
  max_steps: -1                      # -1 to use epochs instead
  warmup_ratio: 0.03                 # Warmup ratio
  weight_decay: 0.01                 # Weight decay
  max_grad_norm: 1.0                 # Gradient clipping

# Sequence configuration
sequence:
  max_length: 16384                  # Maximum sequence length (reduce if OOM)
  packing: false                     # Pack multiple docs per sequence

# Checkpointing
checkpointing:
  save_steps: 500
  save_total_limit: 3
  logging_steps: 10
  eval_steps: 500

# Hardware optimization
hardware:
  gradient_checkpointing: "unsloth"  # Unsloth optimized checkpointing
  bf16: true                         # Use bfloat16
  dataloader_num_workers: 4

# Optimizer (Unsloth-compatible)
optimizer: "adamw_8bit"

# Logging
logging:
  report_to: "none"                  # none, wandb, tensorboard
  run_name: "dapt-gptoss-ts"

# Corpus building settings
corpus:
  # File extensions to include
  extensions:
    - .ts
    - .tsx
    - .js
    - .jsx
    - .mjs
    - .cjs
    - .json
    - .md

  # Maximum file size (bytes)
  max_file_size: 1048576  # 1MB

  # Chunking
  chunk_size: 16384       # Tokens per chunk (match sequence.max_length)
  chunk_overlap: 1024     # Overlap between chunks

  # Deduplication
  dedupe_threshold: 0.8   # Jaccard similarity threshold
  dedupe_num_perm: 128    # MinHash permutations
